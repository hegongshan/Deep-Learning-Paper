### Deep Learning (DL)

#### Theory

* [1986 Nature] BP: **Learning representations by back-propagating errors**. [[PDF](https://www.nature.com/articles/323533a0.pdf)]
* [1989] **Multilayer feedforward networks are universal approximators**.

##### Convolutional Neural Networks (CNNs)

* [2012 NIPS] AlexNet: **ImageNet Classification with Deep Convolutional Neural Networks**. [[PDF](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)]
* [2015 ICLR] VGG: **Very Deep Convolutional Networks for Large-Scale Image Recognition**. [[PDF](https://arxiv.org/pdf/1409.1556.pdf)]
* [2016 CVPR] ResNet: **Deep Residual Learning for Image Recognition**. [[PDF](https://arxiv.org/pdf/1512.03385.pdf)]

### Attention

* [2015 ICLRR] **Neural Machine Translation by Jointly Learning to Align and Translate**. [[PDF](https://arxiv.org/pdf/1409.0473.pdf)]
* [2017 NIPS] **Attention is All you Need**. [[PDF](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)]

#### Overfitting

* [2014 JMLR] **Dropout: A Simple Way to Prevent Neural Networks from Overfitting**. [[PDF](https://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)]
* [2015 JMLR] **Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift**. [[PDF](http://proceedings.mlr.press/v37/ioffe15.pdf)]

#### Stochastic Optimization

* [2011 JMLR] Aadgrad: **Adaptive Subgradient Methods for Online Learning and Stochastic Optimization**. [[PDF](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)]
* [2012] **rmsprop: Divide the gradient by a running average of its recent magnitude**. [[PDF](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)]
* [2012 arXiv] **ADADELTA: An Adaptive Learning Rate Method**. [[PDF](https://arxiv.org/pdf/1212.5701.pdf)]
* [2015 ICLR] **Adam: A Method for Stochastic Optimization**. [[PDF](https://arxiv.org/pdf/1412.6980.pdf)]
